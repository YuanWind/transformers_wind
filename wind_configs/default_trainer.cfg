[Trainer]
# The output directory where the model predictions and checkpoints will be written.
output_dir= tmp
# "Overwrite the content of the output directory. "
# "Use this to continue training if output_dir points to a checkpoint directory."
overwrite_output_dir= false
do_train= false
do_eval= false
do_predict= false
evaluation_strategy= no
prediction_loss_only= false
per_device_train_batch_size= 8
per_device_eval_batch_size= 8
gradient_accumulation_steps= 1
# "Number of predictions steps to accumulate before moving the tensors to the CPU."
eval_accumulation_steps= None
# Number of epochs or steps to wait for before the first evaluation can be performed, depending on the evaluation_strategy.
eval_delay= 0
learning_rate= 5e-05
# Weight decay for AdamW if we apply some.
weight_decay= 0.0
# Beta1 for AdamW optimizer
adam_beta1= 0.9
# Beta2 for AdamW optimizer
adam_beta2= 0.999
# Epsilon for AdamW optimizer.
adam_epsilon= 1e-08
# Max gradient norm.
max_grad_norm= 1.0
num_train_epochs= 3.0
# If > 0: set total number of training steps to perform. Override num_train_epochs.
max_steps= -1
lr_scheduler_type= linear
# Linear warmup over warmup_ratio fraction of total steps.
warmup_ratio= 0.0
# Linear warmup over warmup_steps.
warmup_steps= 0
# Logger log level to use on the main node. Possible choices are the log levels as strings: 'debug', 'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and lets the application set the level. Defaults to 'passive'.
log_level= -1
# Logger log level to use on replica nodes. Same choices and defaults as ``log_level``
log_level_replica= -1
# When doing a multinode distributed training, whether to log once per node or just once on the main node.
log_on_each_node= true
# Tensorboard log dir.
# logging_dir= tmp/runs/Apr12_20-01-34_gpu09
# [no, steps, epoch]
logging_strategy= steps
logging_first_step= false
logging_steps= 500
logging_nan_inf_filter= true
# [no, steps, epoch]
save_strategy= steps
save_steps= 500
save_total_limit= 3
save_on_each_node= false
no_cuda= false
seed= 42
data_seed= None
# Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA architecture. This is an experimental API and it may change.
bf16= false
# Whether to use fp16 (mixed) precision instead of 32-bit
fp16= false
fp16_opt_level= O1
# ["auto", "amp", "apex"] "The backend to be used for half precision."
half_precision_backend= auto
# Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may change.
bf16_full_eval= false
# Whether to use full float16 evaluation instead of 32-bit
fp16_full_eval= false
# Whether to enable tf32 mode, available in Ampere and newer GPU architectures. This is an experimental API and it may change.
tf32= None
# For distributed training: local_rank
local_rank= -1
# ["mpi", "ccl"] The backend to be used for distributed training on Intel XPU.
xpu_backend= None
# TPU: Number of TPU cores (automatically passed by launcher script)
tpu_num_cores= None
# Whether or not to enable debug mode. Current options: "
# "`underflow_overflow` (Detect underflow and overflow in activations and weights), "
# "`tpu_metrics_debug` (print debug metrics on TPU).
debug= []
dataloader_drop_last= false
eval_steps= None
dataloader_num_workers= 0
# If >=0, uses the corresponding part of the output as the past state for next step.
past_index= -1
# An optional descriptor for the run. Notably used for wandb logging.
run_name= tmp
# Whether or not to disable the tqdm progress bars.
disable_tqdm= false
# Remove columns not required by the model when using an nlp.Dataset.
remove_unused_columns= true
# The list of keys in your dictionary of inputs that correspond to the labels.
label_names= None
load_best_model_at_end= false
metric_for_best_model= None
# Whether the `metric_for_best_model` should be maximized or not.
greater_is_better= None
# When resuming training, whether or not to skip the first epochs and batches to get to the same training data.
ignore_data_skip= false
# Whether or not to use sharded DDP training (in distributed training only). The base option
sharded_ddp= []
# Enable deepspeed and pass the path to deepspeed json config file (e.g. ds_config.json) or an already loaded json file as a dict
deepspeed= None
# The label smoothing epsilon to apply (zero means no label smoothing).
label_smoothing_factor= 0.0
optim= adamw_hf
# Whether or not to replace AdamW by Adafactor.
adafactor= false
group_by_length= false
# # column name with precomputed lengths to use when grouping by length.
length_column_name= length
# The list of integrations to report the results and logs to.
report_to= [tensorboard]
# "When using distributed training, the value of the flag `find_unused_parameters` passed to DistributedDataParallel
ddp_find_unused_parameters= None
# When using distributed training, the value of the flag `bucket_cap_mb` passed to DistributedDataParallel
ddp_bucket_cap_mb= None
# Whether or not to pin memory for DataLoader.
dataloader_pin_memory= true
# Whether or not to skip adding of memory profiler reports to metrics.
skip_memory_metrics= true
# Whether or not to use the legacy prediction_loop in the Trainer.
use_legacy_prediction_loop= false
push_to_hub= false
# The path to a folder with a valid checkpoint for your model.
resume_from_checkpoint= None
hub_model_id= None
hub_strategy= every_save
hub_token= <HUB_TOKEN>
hub_private_repo= false
# If True, use gradient checkpointing to save memory at the expense of slower backward pass.
gradient_checkpointing= false
# Whether or not the inputs will be passed to the `compute_metrics` function.
include_inputs_for_metrics= false
push_to_hub_model_id= None
push_to_hub_organization= None
push_to_hub_token= <PUSH_TO_HUB_TOKEN>
# [None, fgm, pgd]
adversarival_type = None
fgm_e= 1.0
pgd_e= 1.0
pgd_a= 1.0
pgd_K= 3




