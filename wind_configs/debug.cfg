[Trainer]
# The output directory where the model predictions and checkpoints will be written.
output_dir = data_wind/output_dir/output_${Save:postfix}
resume_from_checkpoint= None
num_train_epochs= 17
do_train= true
do_eval= true
do_predict= true
# None, steps, epoch
evaluation_strategy= steps
eval_steps= 10
per_device_train_batch_size= 16
gradient_accumulation_steps= 1
per_device_eval_batch_size= 64
eval_accumulation_steps= 4
seed= 777
fp16= True
no_cuda= false
# simple zero_dp_2 zero_dp_3 offload auto_wrap中的一个
sharded_ddp= []
# "When using distributed training, the value of the flag `find_unused_parameters` passed to DistributedDataParallel
ddp_find_unused_parameters= None
# When using distributed training, the value of the flag `bucket_cap_mb` passed to DistributedDataParallel
ddp_bucket_cap_mb= None
# The list of keys in your dictionary of inputs that correspond to the labels.
load_best_model_at_end= True
metric_for_best_model= F1
# 决定 metric_for_best_model 是越大越好，还是越小越好。比如loss是越小越好
greater_is_better= True
label_names= None

# ----------------------------- tricks ----------------------------------

# The label smoothing epsilon to apply (zero means no label smoothing).
label_smoothing_factor= 0.0
# [None, fgm, pgd]
adversarival_type = fgm
emb_name = emb
fgm_e= 0.5
pgd_e= 0.5
pgd_a= 0.5
pgd_k= 3
dataloader_pin_memory= true
dataloader_num_workers= 2

# ----------------- Optimizer and Scheduler args -------------------------

learning_rate= 3e-05
# Weight decay for AdamW if we apply some.
weight_decay= 0.0
# Beta1 for AdamW optimizer
adam_beta1= 0.9
# Beta2 for AdamW optimizer
adam_beta2= 0.999
# Epsilon for AdamW optimizer.
adam_epsilon= 1e-08
# Max gradient norm.
max_grad_norm= 1.0
lr_scheduler_type= linear
# Linear warmup over warmup_ratio fraction of total steps.
warmup_ratio= 0.0
# Linear warmup over warmup_steps.
warmup_steps= 0

# -----------------------------logging and checkpoints args -------------------------------
# [no, steps, epoch]
logging_strategy= steps
logging_first_step= false
logging_steps= 100
logging_nan_inf_filter= true
disable_tqdm= False
report_to= [tensorboard]
# Whether or not to skip adding of memory profiler reports to metrics.
skip_memory_metrics= true
optim= adamw_hf
# Whether or not to replace AdamW by Adafactor.
adafactor= false
save_strategy= ${evaluation_strategy} 
save_steps= ${eval_steps}
save_total_limit= 3
save_on_each_node= false
# If True, use gradient checkpointing to save memory at the expense of slower backward pass.
gradient_checkpointing= false

[Save]
postfix = baseline
temp_dir = ${Trainer:output_dir}/temp_dir
best_model_file = ${Trainer:output_dir}/best_model/best.pt
log_file = ${Trainer:output_dir}/log_${postfix}.txt

[Network]
head_size = 64
hidden_size = 768
delta = -1
alpha = 4e-5
mtl_w = -1
threshold = -1e-2
add_type_emb = True
hidden_dropout_prob = 0.1
attention_probs_dropout_prob = 0.1
max_position_embeddings = 128
max_relative_position = 32

[data]
vocab_file = data_wind/temp_data/vocab.pkl
pretrained_model_name_or_path = data_wind/nezha_30w
eval_loop_file = predict.txt
entity_vocab = data_wind/temp_data/entity_vocab.txt
max_seq_len = 128
max_train_num = -1
max_dev_num = -1







